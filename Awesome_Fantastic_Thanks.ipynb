{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqbnR5BhDLGY"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "Jeremy Glasser - Build Week Project Unit 2\n",
    "Explore and experiment with select market data and linear regression.\n",
    "Looking for features that correlate to large changes in absolute value by date.\n",
    "End goal: check related time-series data from other sources given the highest change dates. \n",
    "Looking for what was seen that caused the correction, or crash in the cases of the obvious Black Swan type events.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_Hv_hwCMDLGa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\17708\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\matplotlib\\\\_image.cp37-win_amd64.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\17708\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\matplotlib\\\\_image.cp37-win_amd64.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\17708\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\matplotlib\\\\_image.cp37-win_amd64.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement talib (from versions: none)\n",
      "ERROR: No matching distribution found for talib\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\17708\\appdata\\roaming\\python\\python37\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install -q --user category_encoders==2.*\n",
    "!pip install -q --user pandas-profiling==2.*\n",
    "!pip install -q --user lazypredict\n",
    "!pip install -q --user Quandl\n",
    "!pip install -q --user pdpbox\n",
    "!pip install -q --user shap\n",
    "!pip install -q --user matplotlib\n",
    "!pip install -q --user talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K18kckZC1VEM",
    "outputId": "9f565c17-015a-4def-91e9-e451a939d3da"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_api' from 'matplotlib' (C:\\Users\\17708\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10488/36410004.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Importing Viz:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;31m# cbook must import matplotlib only within function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;31m# definitions, so it is safe to import from it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatplotlibDeprecationWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanitize_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmplDeprecation\u001b[0m  \u001b[1;31m# deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_api' from 'matplotlib' (C:\\Users\\17708\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Importing main packages:\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Importing Viz:\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, IntSlider, FloatSlider\n",
    "\n",
    "# Importing Model Types:\n",
    "\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuitCV\n",
    "\n",
    "#Importing Model Split/Subsets for Training/Testing:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # Hyperparameter tuning\n",
    "\n",
    "# Importing Pipeline components:\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import OrdinalEncoder\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "# Importing Metrics for Model explanability\n",
    "\n",
    "from sklearn import metrics\n",
    "from math import floor, ceil\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_score, validation_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import shap\n",
    "\n",
    "from talib import add_all_ta_features\n",
    "\n",
    "# Data Import from Fed, and custom financial instruments/assets dataset from my Github:\n",
    "import quandl\n",
    "\n",
    "DATA_PATH = 'https://raw.githubusercontent.com/Artifice-Shell/U2BlackSwan/main/'\n",
    "\n",
    "#mydata = quandl.get(\"FRED/GDP\")\n",
    "#quandl.ApiConfig.api_key = \"tNMeunHf8-3SD87apRvF\"\n",
    "\n",
    "#quandl.bulkdownload(\"DatasetName\")\n",
    "\n",
    "#Retrieve entire Table: (Returns max 1M rows)\n",
    "#quandl.get_table('MER/F1', paginate=True)\n",
    "\n",
    "#Export to Zip to get more data rows:\n",
    "#quandl.export_table('MER/F1')\n",
    "#Put in in a specific place:\n",
    "#quandl.export_table('MER/F1', filename='/my/path/db.zip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quandl - Report/Name\n",
    "#REPORT\tNAME\n",
    "#H.3\tAggregate Reserves of Depository Institutions and the Monetary Base\n",
    "#E.15\tAgricultural Finance Databook\n",
    "#H.8\tAssets and Liabilities of Commercial Banks in the U.S.\n",
    "#Charge-Off and Delinquency Rates on Loans and Leases at Commercial Banks\t\n",
    "#Commercial Paper\t\n",
    "#G.19\tConsumer Credit\n",
    "#Corporate Medium-Term Notes\t\n",
    "#E.16\tCountry Exposure Lending Survey\n",
    "#H.4.1\tFactors Affecting Reserve Balances\n",
    "#Z.1\tFinancial Accounts of the United States\n",
    "#G.20\tFinance Companies\n",
    "#H.10\tForeign Exchange Rates\n",
    "#Household Debt Service and Financial Obligations Ratios\t\n",
    "#G.17\tIndustrial Production and Capacity Utilization\n",
    "#International Summary Statistics\t\n",
    "#H.6\tMoney Stock Measures\n",
    "#Mortgage Debt Outstanding\t\n",
    "#H.15\tSelected Interest Rates\n",
    "#Survey of Consumer Finances\t\n",
    "#E.2\tTerms of Business Lending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCc3XZEyG3XV"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 1*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Define ML problems\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your decisions.\n",
    "\n",
    "- [x] Choose your target. Which column in your tabular dataset will you predict?\n",
    "      Starting with 'change_abs' - looking to identify what is most strongly correlated to the largest values there.\n",
    "- [x] Is your problem regression or classification?\n",
    "      It is a regression problem.\n",
    "- [ ] How is your target distributed?\n",
    "    - Classification: How many classes? Are the classes imbalanced?\n",
    "    - Regression: Is the target right-skewed?\n",
    "- [ ] Choose your evaluation metric(s).\n",
    "    - Classification: Is your majority class frequency >= 50% and < 70% ? If so, you can just use accuracy if you want. Outside that range, accuracy could be misleading. What evaluation metric will you choose, in addition to or instead of accuracy?\n",
    "    - Regression: Will you use mean absolute error, root mean squared error, R^2, or other regression metrics?\n",
    "- [ ] Choose which observations you will use to train, validate, and test your model.\n",
    "    - Are some observations outliers? Will you exclude them?\n",
    "    - Will you do a random split or a time-based split?\n",
    "- [ ] Begin to clean and explore your data.\n",
    "- [ ] Begin to choose which features, if any, to exclude. Would some features \"leak\" future information?\n",
    "\n",
    "If you haven't found a dataset yet, do that today. [Review requirements for your portfolio project](https://lambdaschool.github.io/ds/unit2) and choose your dataset.\n",
    "\n",
    "Some students worry, ***what if my model isn't “good”?*** Then, [produce a detailed tribute to your wrongness. That is science!](https://twitter.com/nathanwpyle/status/1176860147223867393)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFqiif2nDLGb"
   },
   "source": [
    "# Generating the Hypothesis: \n",
    "\n",
    "***'Or how I learned to love possibly 'Boiling the Ocean' to search for the needle in the Black Swan stack, but not yet...'***\n",
    "\n",
    "## What I'm looking at, and why I'm looking at it:\n",
    "\n",
    "I am looking at the time-series data extending back as far as it existed, for multiple asset types, and Indexes related to the US Financial Markets and Currency Values. \n",
    "\n",
    "The assets I chose initially were based on a basket of things I have noted over my lifetime have a tendency to react in a volatile way to certain economic triggers or flashpoints. A needle that pierced all their hearts? A common thread? Who is the hunter? Let's see what we can see.\n",
    "\n",
    "A good PDF sample of what a project/outcome should look like - and more information on the type of quantitative/prescriptive modeling I want to build, and will be doing (when I'm this good at it): https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3538891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "yIbkYU-4DLGc",
    "outputId": "c8d6706c-0236-450e-b005-f5d8852b90e8"
   },
   "outputs": [],
   "source": [
    "#Not wrangling at present - only running a couple new libraries to review the data.\n",
    "\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH+'master.csv')\n",
    "dfSMA = pd.read_csv(DATA_PATH+'master.csv')\n",
    "df['asset'] = df['asset'].astype(\"category\")\n",
    "df[\"class\"] = df[\"class\"].astype(\"category\")\n",
    "\n",
    "dfSMA['asset'] = dfSMA['asset'].astype(\"category\")\n",
    "dfSMA[\"class\"] = dfSMA[\"class\"].astype(\"category\")\n",
    "\n",
    "cols = ['open', 'high', 'low', 'close', 'adj_close', 'vol']\n",
    "\n",
    "# Setting a multi-index on the date and the asset (This will be important in later versions.)\n",
    "df = df.set_index(['date','asset'])\n",
    "\n",
    "# Setting a single index on the date for the SMA and other metrics.\n",
    "#dfSMA = dfSMA.set_index(['date'])\n",
    "\n",
    "# pass them to df.replace(), specifying each char and it's replacement:\n",
    "df[cols] = df[cols].replace({'\\$': '', ',': ''}, regex=True)\n",
    "dfSMA[cols] = dfSMA[cols].replace({'\\$': '', ',': ''}, regex=True)\n",
    "\n",
    "# Printing the head for the main df:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No difference here yet:\n",
    "dfSMA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSMA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4fTZc2aD2dp"
   },
   "outputs": [],
   "source": [
    "df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "dfSMA[cols] = dfSMA[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8roH8ZtHphcG",
    "outputId": "c3963c79-8b2e-464d-9a1d-903eb9866244"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "DvWaALl3EoAE",
    "outputId": "530acc85-488c-4c5b-c48e-0a7d18bdc851",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here I'm doing a little bit of the initial feature engineering for the datasets. \n",
    "# Metrics like: \n",
    "#  Change in value from open to close\n",
    "#  Quantity of range between high and low\n",
    "#  The Trading Volume represented as a logistic value.\n",
    "\n",
    "df['spread'] = df['high'] - df['low']\n",
    "df['change'] = df['open'] - df['close']\n",
    "df['change_abs'] = df['change'].abs()\n",
    "df = df.dropna()\n",
    "\n",
    "df['vol_log'] = df.vol.apply(np.log)\n",
    "df['prior_vol_chng'] = df.groupby('asset').vol.diff()\n",
    "df['20_day_change'] = df.groupby('asset').vol.diff(20)\n",
    "\n",
    "df.iloc[51:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSMA = dfSMA.dropna()\n",
    "\n",
    "dfSMA['spread'] = dfSMA['high'] - dfSMA['low']\n",
    "dfSMA['change'] = dfSMA['open'] - dfSMA['close']\n",
    "dfSMA['change_abs'] = dfSMA['change'].abs()\n",
    "\n",
    "# Using the alternate single index dataframe for SMA, I can now see the SMA_20, and SMA_50.\n",
    "\n",
    "dfSMA['SMA_20'] = talib.SMA(dfSMA['close'], timeperiod=20)\n",
    "dfSMA['SMA_50'] = talib.SMA(dfSMA['close'], timeperiod=50)\n",
    "\n",
    "# Adding in the Exponential Moving Average as well:\n",
    "\n",
    "dfSMA['EMA_20'] = talib.EMA(dfSMA['close'], timeperiod=20)\n",
    "dfSMA['EMA_50'] = talib.EMA(dfSMA['close'], timeperiod=50)\n",
    "\n",
    "# Adding in the log of the volume here also:\n",
    "dfSMA['vol_log'] = dfSMA.vol.apply(np.log)\n",
    "dfSMA.iloc[51:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A categorical scatterplot based on the asset's Class: Here using Volume on the vertical, with change_abs on the horizontal.\n",
    "\n",
    "fig = px.scatter(dfSMA, x = 'SMA_20', y = 'change_abs', color = 'class', trendline=\"ols\")\n",
    "fig.update_xaxes(range=[0, 15000])\n",
    "fig.update_yaxes(range=[0, 500])\n",
    "fig.show()\n",
    "\n",
    "results = px.get_trendline_results(fig)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A categorical scatterplot based on the asset's Class: Here using Volume on the vertical, with change_abs on the horizontal.\n",
    "\n",
    "fig = px.scatter(dfSMA, x = 'SMA_50', y = 'change_abs', color = 'class', trendline=\"ols\")\n",
    "fig.update_xaxes(range=[0, 15000])\n",
    "fig.update_yaxes(range=[0, 500])\n",
    "fig.show()\n",
    "\n",
    "results = px.get_trendline_results(fig)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really ugly and useless chart, but I made it anyway.\n",
    "\n",
    "plt.plot(dfSMA['close'], color='blue', label='Daily Close Price')\n",
    "plt.plot(dfSMA['SMA_20'], color='green', label='SMA 20')\n",
    "plt.plot(dfSMA['SMA_50'], color='red', label='SMA 50')\n",
    "plt.legend()\n",
    "plt.title('Simple Moving Averages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the dataset with a facet grid: (I made one, but I broke it.)\n",
    "\n",
    "#sns.catplot(x=\"vol_log\", y=\"change_abs\", kind=\"boxen\",data=df.sort_values(\"class\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL7ZPCU-DLGc"
   },
   "source": [
    "# II. Split Data\n",
    "\n",
    "**Task 2:** Splitting dataFrame `df` into a feature matrix `X` and the target vector `y`. Predicting `'change_abs'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYTL-C1ADLGd"
   },
   "outputs": [],
   "source": [
    "target = 'change_abs'\n",
    "drop_cols = ['open', 'high', 'low', 'close', 'adj_close','change','vol']\n",
    "\n",
    "y1 = df[target]\n",
    "X1 = df.drop(columns = target)\n",
    "\n",
    "y2 = dfSMA[target]\n",
    "X2 = dfSMA.drop(columns = target)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: -inf's in the logs - adding pd.set_option('use_inf_as_na', True) up top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X1.drop(columns = drop_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=.3, random_state=42)\n",
    "\n",
    "X2 = X2.drop(columns = drop_cols)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X2, y2, test_size=.4, random_state=42)\n",
    "X1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('use_inf_as_na', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic1oWkEADLGd"
   },
   "source": [
    "**Task 3:** Using a randomized split, divide `X` and `y` into a training set (`X_train`, `y_train`) and a validation set (`X_val`, `y_val`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "NkzmwbKtJ0lx",
    "outputId": "dea44fa4-bea4-445d-ade9-30b34003ca29",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POalDKEkKPJW",
    "outputId": "57b8ed44-c002-418e-9f50-f9904a29f97c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commenting out as results take considerable time, and have been documented in a cell below.\n",
    "\n",
    "#reg = LazyRegressor(predictions=True)\n",
    "#models_reg, predictions = reg.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results of the LazyRegressor, put into a field to make copy/paste work for Excel.\n",
    "'''\tAdjusted R-Squared\tR-Squared\tRMSE\tTime Taken\n",
    "Model\t\t\t\t\n",
    "GradientBoostingRegressor\t0.80\t0.80\t14.57\t7.71\n",
    "MLPRegressor\t0.79\t0.79\t14.81\t22.61\n",
    "OrthogonalMatchingPursuit\t0.79\t0.79\t14.82\t0.06\n",
    "HuberRegressor\t0.79\t0.79\t14.83\t0.75\n",
    "LassoCV\t0.79\t0.79\t14.83\t0.54\n",
    "LinearSVR\t0.79\t0.79\t14.84\t13.08\n",
    "RidgeCV\t0.79\t0.79\t14.84\t0.09\n",
    "Ridge\t0.79\t0.79\t14.84\t0.07\n",
    "BayesianRidge\t0.79\t0.79\t14.84\t0.09\n",
    "TransformedTargetRegressor\t0.79\t0.79\t14.84\t0.07\n",
    "LinearRegression\t0.79\t0.79\t14.84\t0.06\n",
    "Lars\t0.79\t0.79\t14.84\t0.08\n",
    "LassoLarsIC\t0.79\t0.79\t14.84\t0.08\n",
    "LarsCV\t0.79\t0.79\t14.84\t0.18\n",
    "LassoLarsCV\t0.79\t0.79\t14.84\t0.16\n",
    "OrthogonalMatchingPursuitCV\t0.79\t0.79\t14.84\t0.14\n",
    "Lasso\t0.79\t0.79\t14.85\t0.07\n",
    "ElasticNetCV\t0.79\t0.79\t14.86\t0.56\n",
    "SGDRegressor\t0.79\t0.79\t14.95\t0.16\n",
    "RandomForestRegressor\t0.79\t0.79\t15.08\t27.52\n",
    "KNeighborsRegressor\t0.79\t0.79\t15.12\t1.26\n",
    "RANSACRegressor\t0.78\t0.78\t15.32\t0.19\n",
    "XGBRegressor\t0.78\t0.78\t15.33\t3.45\n",
    "BaggingRegressor\t0.77\t0.77\t15.57\t2.86\n",
    "ExtraTreesRegressor\t0.76\t0.76\t15.86\t14.89\n",
    "AdaBoostRegressor\t0.75\t0.75\t16.29\t1.88\n",
    "HistGradientBoostingRegressor\t0.71\t0.71\t17.70\t0.67\n",
    "ElasticNet\t0.70\t0.70\t17.79\t0.07\n",
    "LGBMRegressor\t0.70\t0.70\t17.84\t0.40\n",
    "ExtraTreeRegressor\t0.65\t0.65\t19.38\t0.21\n",
    "DecisionTreeRegressor\t0.64\t0.64\t19.59\t0.47\n",
    "GeneralizedLinearRegressor\t0.61\t0.61\t20.38\t0.21\n",
    "TweedieRegressor\t0.61\t0.61\t20.38\t0.19\n",
    "NuSVR\t0.61\t0.61\t20.40\t1514.61\n",
    "SVR\t0.61\t0.61\t20.40\t1096.83\n",
    "PoissonRegressor\t0.20\t0.20\t29.27\t0.28\n",
    "PassiveAggressiveRegressor\t0.15\t0.15\t30.12\t0.14\n",
    "LassoLars\t-0.00\t-0.00\t32.65\t0.06\n",
    "DummyRegressor\t-0.00\t-0.00\t32.65\t0.06'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Pipeline based models to compare based on what LazyPredict recommended:\n",
    "\n",
    "*I'm aware that this makes the plotting more complicated, and I can't seem to find regression samples where we plotted from Pipelines, I am only finding those on Classification Models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP = make_pipeline(\n",
    "    OrdinalEncoder(), \n",
    "    SimpleImputer(), \n",
    "    MLPRegressor()\n",
    ")\n",
    "\n",
    "model_MLP.fit(X_train, y_train)\n",
    "\n",
    "expected_y  = y_test\n",
    "predicted_y = model_MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP_score = model_MLP.score(X_test, y_test)\n",
    "test_set_rmse = (np.sqrt(mean_squared_error(expected_y, predicted_y)))\n",
    "\n",
    "###OPTIONAL TO print('Linear Regression R^2:', model_lr_r2_score)\n",
    "print(metrics.r2_score(expected_y, predicted_y)),\n",
    "print(metrics.mean_squared_error(expected_y, predicted_y))\n",
    "print(model_MLP_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GBR = make_pipeline(\n",
    "    OrdinalEncoder(), \n",
    "    SimpleImputer(), \n",
    "    GradientBoostingRegressor()\n",
    ")\n",
    "\n",
    "model_GBR.fit(X_train, y_train)\n",
    "\n",
    "expected_y  = y_test\n",
    "y_pred = model_GBR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3KxAvJ5DLGe"
   },
   "source": [
    "# A few different Pipelines to compare:\n",
    "\n",
    "Built out some of the higher scoring models from the LazyPredict Regressor to step through them and compare them from their pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model_lr = make_pipeline(\n",
    "    OneHotEncoder(), \n",
    "    SimpleImputer(),\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    LinearRegression()\n",
    "    )\n",
    "\n",
    "model_GBR = make_pipeline(\n",
    "    OneHotEncoder(), \n",
    "    SimpleImputer(),\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    GradientBoostingRegressor()\n",
    "    )\n",
    "\n",
    "model_RFR = make_pipeline(\n",
    "    OneHotEncoder(), \n",
    "    SimpleImputer(),\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    RandomForestRegressor()\n",
    "    )\n",
    "\n",
    "model_MLP = make_pipeline(\n",
    "    OneHotEncoder(), \n",
    "    SimpleImputer(),\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    MLPRegressor()\n",
    "    )\n",
    "\n",
    "model_RCV = make_pipeline(\n",
    "    OneHotEncoder(), \n",
    "    SimpleImputer(),\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    RidgeCV()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Fit predictor on the (training) data\n",
    "model_lr.fit(X_train, y_train)\n",
    "model_GBR.fit(X_train, y_train)\n",
    "model_MLP.fit(X_train, y_train)\n",
    "model_RCV.fit(X_train, y_train)\n",
    "model_RFR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Linear Regression Model coefficients:\\n\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(X.columns[i], \"=\", model_lr.named_steps[\"linearregression\"].coef_[i].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Cross-validation Model coefficients:\\n\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(X.columns[i], \"=\", model_RCV.named_steps[\"ridgecv\"].coef_[i].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "#matplotlib.pyplot.ion()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.regplot(expected_y, predicted_y, fit_reg=True, scatter_kws={\"s\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIhh1SbcDLGe"
   },
   "source": [
    "# Baseline MAE - it's the number to beat.\n",
    "\n",
    "Since this is a **Regression** problem we are using the Baseline MAE as the initial accuracy benchmark for the data models to beat. If they cannot, then you need to add more Features/Independent Variables, increase the sample size, address missing values/outliers, try a different model, and maybe all of the above.\n",
    "\n",
    "**tl;dr:** *It's y_train's mean **VS** the mean of the training model's predicted y. It's the mean of the difference (absolute value, no +/- meaning) for each observation for X in X_train.*\n",
    "\n",
    "For Regression the Baseline MAE is calculated as the first metric for evaluating a model's performance. The difference\n",
    "between the trained result's predicted value y (y_pred) given each observation for X in X_train, is compared to the mean for y_train (add or subtract is irrelevant). The difference between the y_train **mean** and the predicted value is expressed as an **absolute** value, and defined as the **error**. \n",
    "\n",
    "We have seen how that works in a tabular dataset, and this is also conceptually easy to seen in Excel as well (which I will not demonstrate here.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "Qk4r5sGeDLGe",
    "outputId": "91b34803-2f40-48b0-80b1-951639107eef"
   },
   "outputs": [],
   "source": [
    "y_pred = [y_train.mean()]*len(y_train)\n",
    "mean_absolute_error(y_train, y_pred)\n",
    "#R squared is zero, since R Squared tells you how good the model is compared to the mean baseline\n",
    "print('R Squared:', r2_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTnsLc4tDLGf"
   },
   "outputs": [],
   "source": [
    "#Defining the y_pred for Regression:\n",
    "\n",
    "y_pred = [y_train.mean()]*len(y_train)\n",
    "\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_train, y_pred)\n",
    "\n",
    "print('Mean Absolute Change:', y_train.mean())\n",
    "print('Baseline MAE:', baseline_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8oTWHqQDLGf"
   },
   "outputs": [],
   "source": [
    "model_rf = make_pipeline(\n",
    "    OrdinalEncoder(), \n",
    "    SimpleImputer(), \n",
    "    RandomForestRegressor()\n",
    ")\n",
    "\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GBR = make_pipeline(\n",
    "    OrdinalEncoder(), \n",
    "    SimpleImputer(), \n",
    "    GradientBoostingRegressor()\n",
    ")\n",
    "\n",
    "model_GBR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-5ti5_4DLGf"
   },
   "source": [
    "# V. Check Metrics\n",
    "\n",
    "**Calculate the training and testing metrics for the models to be compared:**\n",
    "\n",
    "**Using the Baseline Mean Absolute Error (MAE) we calculated earlier, check the Training and Testing MAE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBppA81hDLGf"
   },
   "source": [
    "**Metrics and Results:**\n",
    "\n",
    "Calculate the training and test mean absolute error for the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the Training MAE to the Test MAE for the Random Forest Regressor:\n",
    "\n",
    "training_mae_rf = mean_absolute_error(y_train, model_rf.predict(X_train)).round(3)\n",
    "test_mae_rf = mean_absolute_error(y_test, model_rf.predict(X_test)).round(3)\n",
    "\n",
    "print('Training MAE:', training_mae_rf)\n",
    "print('Test MAE:', test_mae_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffonUuuEDLGf"
   },
   "outputs": [],
   "source": [
    "# Comparison of the Training MAE to the Test MAE for the Gradient Boosting Regressor:\n",
    "\n",
    "training_mae = mean_absolute_error(y_train, model_GBR.predict(X_train)).round(3)\n",
    "test_mae = mean_absolute_error(y_test, model_GBR.predict(X_test)).round(3)\n",
    "\n",
    "print('Training MAE:', training_mae)\n",
    "print('Test MAE:', test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Feature Importances for the Random Forest Reg\n",
    "\n",
    "*The most important feature is the spread, which is based on the High/Low, not the Open/Close. So it's not a direct observation, but I would expect it would have a high feature importance, there may be a way to alter it by dividing the volume by it to make a ratio, but I'm not sure if that's worth doing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "get_feat_imp = model_rf.named_steps['randomforestregressor']\n",
    "importances = pd.Series(get_feat_imp.feature_importances_, X_train.columns)\n",
    "\n",
    "# Plot feature importances\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 20\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "get_feat_imp_GBR = model_GBR.named_steps['gradientboostingregressor']\n",
    "importances = pd.Series(get_feat_imp_GBR.feature_importances_, X_train.columns)\n",
    "\n",
    "# Plot feature importances\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 20\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the case of the Gradient Boosting Regressor, it doesn't seem to consider the volume as important, let's try a different viz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 7))\n",
    "sns.countplot(y=\"class\", data=df, color=\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the models compared using MAE:\n",
    "\n",
    "**Random Forest Regressor:** shows a significant difference between the training and testing mean absolute error, which would mean it didn't do as well on the test data as it did on the training data. The absolute error quantitatively in terms of the dataset is not a large one, but less error in Quantitative Financial Analytics is clearly better, and if you're expecting to be off by ~$1.07, and you are nearly ~$2.7 off, depending on the strategy employed by the trading bot, you may be out of a whole lot of cash even though it's not a $500 difference.\n",
    "\n",
    "**Gradient Boosting Regressor:** shows a much smaller different between the training and testing MAE, as close as to be no difference at all, though still a slight difference. This could be useful where the Random Forest Regressor's difference doesn't match the risk profile for the usecase.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmNspq4PDLGg"
   },
   "source": [
    "**Comparing R2 scores for the models:**\n",
    "\n",
    "Calculate the training and test  R2  score for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the Training R2 to the Test MAE for the Random Forest Regressor:\n",
    "\n",
    "training_rf_r2 = mean_squared_error(y_train, model_rf.predict(X_train), squared=False).round(3)\n",
    "test_rf_r2 = mean_squared_error(y_test, model_rf.predict(X_test), squared=False).round(3)\n",
    "\n",
    "print('Random Forest - Training Mean Squared Error / R2:', training_rf_r2)\n",
    "print('Random Forest - Test Mean Squared Error / R:', test_rf_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the Training MAE to the Test MAE for the Gradient Boosting Regressor:\n",
    "\n",
    "\n",
    "training_r2 = mean_squared_error(y_train, model_GBR.predict(X_train), squared=False).round(3)\n",
    "test_r2 = mean_squared_error(y_test, model_GBR.predict(X_test), squared=False).round(3)\n",
    "\n",
    "print('Gradient Boosted - Training Mean Squared Error / R', training_r2)\n",
    "print('Gradient Boosted - Test Mean Squared Error / R:', test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison using the R2:\n",
    "\n",
    "**Random Forest Regressor**: Similar theme here as with the MAE given that it's related that should be no surprise.\n",
    "\n",
    "**Gradient Boosting Regressor**: Again the performance of the Gradient Boosting Regressor between the training and the testing was a tighter spread, however, it was a higher value - so there's a clear trade-off between the two models and the choice of which to use would come down to strategy (or insanity of using either.)\n",
    "\n",
    "**Overall:** Since the Absolute Error is low for both, I could see where this would be useful, however there is more to this than meets the eye, and I'll be getting to that in later revisions. Chiefly, the Time-Series nature of the predictions. If the predicted absolute change is based on a prior period, and when modeled it demonstrates that it cannot be safely acted on, then there would be no value in the prediction. The ultimate goal of anything like this is to create a 'leading indicator', not a 'lagging indicator' for reasons that are clear to any investor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73Rh_X96DLGg"
   },
   "source": [
    "# Model Performances\n",
    "\n",
    "**Overall:** Since the Absolute Error is relatively low for both, I could see where this could be useful. However there is more to this than meets the eye, and I'll be getting to that in later revisions. \n",
    "\n",
    "The primary caveat is that this is ultimately only useful when looked at through the time-series lens based on the nature of the predictions and the intent of predicting anything at all.\n",
    "\n",
    "If the predicted absolute change is based on a prior period, and when modeled it demonstrates that it cannot be safely acted on, then there would be no value in the prediction. \n",
    "\n",
    "The ultimate goal of anything of this sort is to create a 'leading indicator', not a 'lagging indicator' for reasons that are clear to any investor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIhvlBHdDLGh"
   },
   "source": [
    "#MORE COMMENTARY IF NEEDED.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7N0W8znDLGh"
   },
   "source": [
    "# Future Plans (before running away from this dataset):\n",
    "\n",
    "** IDK... figure something out here... or don't because if it was this simple, we'd all be rich, or just the people who solved it would be. It's Chaos Theory, it's all covered by Numberphile and Veritasium, and ThreeBlue1Brown, and many more... so maybe there's a secret in here somewhere, and I just have to be crazy enough to try to find it.\n",
    "\n",
    "**Note:** I've done pretty crazy things before, and gotten good results from it - sometimes not knowing better is all the difference there is between a lucky autodidact, and an unlucky Professor. One of them was dumb enough to try... and fail repeatedly... until they stopped failing and grew wings. Maybe not today, maybe not tomorrow, maybe not ever... but not not ever. With the right tools and new meaning to measures, what is possible changes - and I'm not above using Quantum Compute resources to throw sand in the eyes of well-heeled Quants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1dVpogcDLGh"
   },
   "outputs": [],
   "source": [
    "#Write More Code or Bust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz of Top Ten Coefficients for Regression:\n",
    "%matplotlib inline\n",
    "from category_encoders import OneHotEncoder\n",
    "from category_encoders import OrdinalEncoder\n",
    "from matplotlib import pyplot\n",
    "model_lr_ns = model_lr.named_steps['linearregression']\n",
    "#encoder_ns = model_MLP.named_steps['ordinalencoder']\n",
    "\n",
    "encoder = OneHotEncoder(use_cat_names=True)\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_val_encoded = encoder.transform(X_test)\n",
    "X_train_encoded.shape, y_train.shape, X_val_encoded.shape, y_test.shape\n",
    "\n",
    "coefficients = pd.Series(model_lr_ns.coef_[0], X_train.columns)\n",
    "features = X.columns\n",
    "feat_imp = pd.Series(coefficients, index=features).abs()\n",
    "\n",
    "feat_imp.tail(5).plot(kind='barh')\n",
    "plt.title('Top Ten Coefficients for Linear Regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Shap for Model Explainability on a RandomForestRegressor.\n",
    "\n",
    "**Now that I've fit a new model for the RandomForestRegressor I will be reviewing that with the Shap Tree Explainer with JS visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''explainer =  shap.Explainer(model_lr.named_steps[\"linearregression\"],masker=shap.maskers.Impute(data=X_train),\n",
    "\n",
    "                                  feature_names=X_train.columns)\n",
    "\n",
    "shap_values = explainer.shap_values(model_lr[:-1].transform(X_train))\n",
    "\n",
    " \n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# plot the SHAP values for the output of the first instance\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:], link=\"logit\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''row = X_train.tail(1)\n",
    "\n",
    "explainer =  shap.LinearExplainer(model_GBR, data=X_train, masker=shap.maskers.Impute(data=X_train),\n",
    "                                  feature_names=X_train.columns, algorithm=\"linear\")\n",
    "shap_values = explainer(X_test, nsamples=100)\n",
    "\n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "\n",
    "shap.force_plot(\n",
    "    base_value = explainer.expected_value,\n",
    "    shap_values = shap_values(X_test, nsamples=100),\n",
    "    features = row\n",
    ")\n",
    "\n",
    "# plot the SHAP values for the Setosa output of the first instance\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"logit\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is running too slowly, skipping it for now:\n",
    "'''explainer =  shap.TreeExplainer(model_RFR.named_steps[\"randomforestregressor\"],masker=shap.maskers.Impute(data=X_train),\n",
    "\n",
    "                                  feature_names=X_train.columns, algorithm=\"linear\")\n",
    "\n",
    "shap_values = explainer.shap_values(model_RFR[:-1].transform(X_train))\n",
    "\n",
    " \n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "# plot the SHAP values for the output of the first instance\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:], link=\"logit\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boosted Regression Model coefficients:\\n\")    \n",
    "for i in range(X.shape[1]):\n",
    "    print(X['class'] [i], \"=\", model_GBR.named_steps[\"gradientboostingregressor\"].feature_importances_[i].round(6))\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(\"Random Forest Regressor Model coefficients:\\n\")    \n",
    "for i in range(X.shape[1]):\n",
    "    print(X.columns[i], \"=\", model_RFR.named_steps[\"randomforestregressor\"].feature_importances_[i].round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multilayer Perceptron (NeuralNet) Model coefficients:\\n\")    \n",
    "for i in range(X.shape[1]):\n",
    "    print(X.columns[i], \"=\", \"\\n\", model_MLP.named_steps[\"mlpregressor\"].get_params(deep=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP.predict(X_test)\n",
    "print(\"\\n\")\n",
    "model_GBR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues with ploting from a pipeline in various ways, attempting to resolve that: \n",
    "\n",
    "Appears to be something to do with the 'class' and 'asset' data colums, and I want it grouped by them.\n",
    "The charts were working in a prior version, not sure what went wrong, but it's 'Str' related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###  As seen here, there is more than one way to achieve the same result, \n",
    "## in this case I used the i.loc within a forloop/list comprehension, and the 'Stock'...\n",
    "#  entry in the class column was forced to be looked at, where it needed to group by them.\n",
    "\n",
    "models_reg[\"R-Squared\"] = [0 if i < 0 else i for i in models_reg.iloc[:,0] ]\n",
    "                            \n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=models_reg.index, y=\"R-Squared\", data=models_reg)\n",
    "ax.set(ylim=(0,1))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(use_cat_names=True)\n",
    "X_train_encoded = encoder.fit_transform(X_train['asset'])\n",
    "X_test_encoded = encoder.transform(X_test['asset'])\n",
    "X_train_encoded.shape, y_train.shape, X_test_encoded.shape, y_test.shape\n",
    "\n",
    "model_GBReg = ensemble.GradientBoostingRegressor(**params)\n",
    "model_GBReg.fit(X_train_encoded, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, model_GBReg.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'ls'}\n",
    "\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(model_GBReg.staged_predict(X_test)):\n",
    "    test_score[i] = model_GBReg.loss_(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, model_GBReg.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.scatter(df, x='vol', y='change_abs',trendline='ols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END... but not really because I'll be done when I'm rich or dead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "231_maybe__but_from_224_assignment_High_Ram_Runtime.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
